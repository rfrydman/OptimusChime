{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OptimusChime: Singer Placement Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.fftpack import fft\n",
    "from scipy.io import wavfile\n",
    "\n",
    "class singer:\n",
    "    def __init__(self, name, vocal_range):\n",
    "        \"\"\"\n",
    "        @name: str, name of singer for indexing\n",
    "        @vocal_range: either tuple of integers (for later classification), or hard-entered str (\"S, A, T, B\")\n",
    "        \"\"\"\n",
    "        self.name = name \n",
    "        self.vocal_range = vocal_range\n",
    "        \n",
    "        #as of right now, these are the only initial inputs I can think of that are necessary to initiliaze\n",
    "        \n",
    "    def read(self, file_path):\n",
    "        \"\"\"\n",
    "        @file_path: str, path to audio recording\n",
    "        \"\"\"\n",
    "        self.sampling_rate, self.audio_data = wavfile.read(file_path)\n",
    "    \n",
    "    def GENERAL_AUDIO_PROCESSING(self):\n",
    "        #catch-all function that will be developed to process data in any way that is required to perform FFT.\n",
    "        \n",
    "        #def normalize_audio(self):\n",
    "            #normalize the audio data to [-1, 1]\n",
    "            \n",
    "        return\n",
    "        \n",
    "    def rory_fft(self, **kwargs):\n",
    "        #This will be a self-coded fast Fourier transform, which I have not started work on yet\n",
    "        return\n",
    "    \n",
    "    def class_fft(self):\n",
    "        input_data = self.GENERAL_AUDIO_PROCESSING() # this is 8-bit track, b is now normalized on [-1,1)\n",
    "        self.frequencies = fft(input_data) # calculate fourier transform (complex numbers list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above class will serve as a general starting point for input and processing data, but as I have not fully implemented the class, I will proceed with some stand-alone functions for the rest of the check-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wavfile\n",
    "import scipy\n",
    "import scipy.fftpack\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(file_path, plot = False):\n",
    "    fs_rate, signal = wavfile.read(file_path)\n",
    "\n",
    "    l_audio = len(signal.shape)\n",
    "    if l_audio == 2:\n",
    "        signal = signal.sum(axis=1) / 2\n",
    "\n",
    "    N = signal.shape[0]\n",
    "\n",
    "    secs = N / float(fs_rate)\n",
    "\n",
    "    Ts = 1.0/fs_rate # sampling interval in time\n",
    "\n",
    "    t = scipy.arange(0, secs, Ts) # time vector as scipy arange field / numpy.ndarray\n",
    "    FFT = abs(scipy.fft(signal))\n",
    "    FFT_side = FFT[range(N//2)] # one side FFT range\n",
    "\n",
    "    freqs = scipy.fftpack.fftfreq(signal.size, t[1]-t[0])\n",
    "    fft_freqs = np.array(freqs)\n",
    "\n",
    "    freqs_side = freqs[range(N//2)] # one side frequency range\n",
    "    fft_freqs_side = np.array(freqs_side)\n",
    "    \n",
    "    if plot == True:\n",
    "        plt.subplot(211)\n",
    "        p1 = plt.plot(t, signal, \"g\") # plotting the signal\n",
    "\n",
    "        plt.subplot(212)\n",
    "        p3 = plt.plot(freqs_side, abs(FFT_side), \"b\") # plotting the positive fft spectrum\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return np.array(fft_freqs_side), np.array(FFT_side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "handel_freq, handel_amp = analyze(\"./recordings/handel.wav\")\n",
    "vivaldi_freq, vivaldi_amp = analyze(\"./recordings/vivaldi.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two above audio files are real singers singing unaccompanied (both are excerpts from classical pieces that I knew had unaccompanied sections). Length doesn't matter as the FFT transform just needs a sample (although I suspect longer recordings are better, as long as the pitch is held constant).\n",
    "\n",
    "One of the key things to note here is that I need the singers' recordings to be in .wav format, and also need the notes to be the same pitch. For the two above recordings, the pitches are completely different, so the analysis wont actually work. Nevertheless, I will continue on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a function to identify fundamental note\n",
    "\n",
    "Although this is labeled data, I still want the environment to have the fundamental frequency to perform the analyses. Finding the fundamental before the harmonics seems reasonable as the fundamental has the highest frequency amplitude, and the identification of harmonics should be identified based on the fundamental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fundamental(freq, amp):\n",
    "    #naive fundamental finder\n",
    "    loc = amp.argmax()\n",
    "    fundamental = freq[loc]\n",
    "    return fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental could be more accurately found by computing area under the frequency amplitude curve around a midpoint. The largest area under the curve would be the fundamental. This would also allow identification of secondary pitches that have large freq amplitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "handel_fund = fundamental(handel_freq, handel_amp)\n",
    "vivaldi_fund = fundamental(vivaldi_freq, vivaldi_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635.8045212765958 596.9082446808511\n"
     ]
    }
   ],
   "source": [
    "print(handel_fund, vivaldi_fund)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def area_fund(freq, amp, tol = 50):\n",
    "    areas = []\n",
    "    freqs_loc = []\n",
    "    i = 0\n",
    "    while i < len(amp):\n",
    "        x_range = freq[i:i+tol]\n",
    "        y_range = amp[i:i+tol]\n",
    "        area = metrics.auc(x_range, y_range)\n",
    "        areas.append(area)\n",
    "        freqs_loc.append(freq[i])\n",
    "        i += tol \n",
    "    areas = np.array(areas)\n",
    "    fund_loc = areas.argmax()\n",
    "    fund = freqs_loc[fund_loc]\n",
    "    return fund"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "handel_fund_1 = area_fund(handel_freq, handel_amp)\n",
    "vivaldi_fund_1 = area_fund(vivaldi_freq, vivaldi_amp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610.8710106382979 585.9375\n"
     ]
    }
   ],
   "source": [
    "print(handel_fund_1, vivaldi_fund_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do:\n",
    "\n",
    "1. Calculate the harmonics\n",
    "2. Create clean superposed sine wave models of singers voices\n",
    "3. Make figures\n",
    "\n",
    "    Animate interferences patterns of different singers\n",
    "    \n",
    "    Plot harmonic power series plot\n",
    "    \n",
    "\n",
    "4. Create a naive algorithm that optimizes vocal placement:\n",
    "\n",
    "    Given a set of N recordings, pair-wise minimize the weighted sum of the differences of the harmonics of two recordings, return the list of sorted recordings\n",
    "    \n",
    "\n",
    "5. Create a better algorithm:\n",
    "\n",
    "    Given the same set of recordings, pair-wise minimize the weighted sum of a cost function over the harmonics of two recordings, return the list of ordered recordings, also consider voice part/gender.\n",
    "    \n",
    "    **COST FUNCTION IDEAS:** The closer the harmonics of the recordings are, the more dissonant it will sound, unless they are within a certain very fine tolerance interval. The naive difference cost function will promote dissonance (ugly beats) over harmony because some singers' harmonics will be minimized, but outside the interval of tolerance. In fact, singers with harmonics very far apart may actually sound \"better\" than ones with harmonics close together. A cost function that evaluates how close each singer's harmonics are to the **_harmonics of the harmonics_** of the other singer, while also severely penalizing values within the dissonance interval. \n",
    "    \n",
    "    \n",
    "6. Final visualization ideas:\n",
    "\n",
    "   Plot of HPS of each singer\n",
    "   \n",
    "   Final choir arrangement plot (with intermediate steps: different choir shapes: list, array (nearest neighbour minimisation)\n",
    "   \n",
    "   Plot \"dissonance\" of each singer (the beat frequency metric)\n",
    "   \n",
    "   More to come as I see what data I end up generating\n",
    "   \n",
    "7. Possible extra tidbits:\n",
    "    \n",
    "    Add considerations of spacing between singers and direction (would incorporate phase, so would likely use clean sinusoidal model)\n",
    "    \n",
    "   \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WANT TO IMPLEMENT SOMETHING LIKE THIS FOR HPS:\n",
    "\n",
    "https://gist.github.com/fasiha/957035272009eb1c9eb370936a6af2eb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
